{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# 1. Data Preparation (same as before)\n",
        "df = pd.read_csv('DailyDelhiClimate.csv')\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df.set_index('date', inplace=True)\n",
        "\n",
        "features = ['meantemp', 'humidity', 'wind_speed', 'meanpressure']\n",
        "df = df[features]\n",
        "\n",
        "scalers = {}\n",
        "for column in df.columns:\n",
        "    scaler = MinMaxScaler()\n",
        "    df[column] = scaler.fit_transform(df[[column]])\n",
        "    scalers['scaler_' + column] = scaler\n",
        "\n",
        "n_past = 10\n",
        "n_future = 5\n",
        "n_features = len(features)\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "for i in range(n_past, len(df) - n_future + 1):\n",
        "    X.append(df.iloc[i - n_past:i].values)\n",
        "    y.append(df.iloc[i:i + n_future].values)\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "train_size = int(len(X) * 0.8)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "# 2. LSTM Encoder-Decoder Models\n",
        "\n",
        "# E1D1 Model with LSTM\n",
        "encoder_inputs_e1d1 = tf.keras.layers.Input(shape=(n_past, n_features))\n",
        "encoder_lstm_e1d1 = tf.keras.layers.LSTM(16, return_state=True)\n",
        "encoder_outputs_e1d1, state_h_e1d1, state_c_e1d1 = encoder_lstm_e1d1(encoder_inputs_e1d1)\n",
        "encoder_states_e1d1 = [state_h_e1d1, state_c_e1d1]\n",
        "\n",
        "decoder_inputs_e1d1 = tf.keras.layers.RepeatVector(n_future)(encoder_outputs_e1d1)\n",
        "decoder_lstm_e1d1 = tf.keras.layers.LSTM(16, return_sequences=True)\n",
        "decoder_outputs_e1d1 = decoder_lstm_e1d1(decoder_inputs_e1d1, initial_state=encoder_states_e1d1)\n",
        "decoder_dense_e1d1 = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(n_features))\n",
        "decoder_outputs_e1d1 = decoder_dense_e1d1(decoder_outputs_e1d1)\n",
        "\n",
        "model_e1d1_lstm = tf.keras.models.Model(encoder_inputs_e1d1, decoder_outputs_e1d1)\n",
        "\n",
        "\n",
        "# E2D2 Model with LSTM (2 layers each)\n",
        "encoder_inputs_e2d2 = tf.keras.layers.Input(shape=(n_past, n_features))\n",
        "\n",
        "encoder_lstm1_e2d2 = tf.keras.layers.LSTM(16, return_sequences=True, return_state=True)\n",
        "encoder_outputs1_e2d2, state_h1_e2d2, state_c1_e2d2 = encoder_lstm1_e2d2(encoder_inputs_e2d2)\n",
        "encoder_states1_e2d2 = [state_h1_e2d2, state_c1_e2d2]\n",
        "\n",
        "encoder_lstm2_e2d2 = tf.keras.layers.LSTM(16, return_state=True)\n",
        "encoder_outputs2_e2d2, state_h2_e2d2, state_c2_e2d2 = encoder_lstm2_e2d2(encoder_outputs1_e2d2)\n",
        "encoder_states2_e2d2 = [state_h2_e2d2, state_c2_e2d2]\n",
        "\n",
        "\n",
        "decoder_inputs_e2d2 = tf.keras.layers.RepeatVector(n_future)(encoder_outputs2_e2d2)\n",
        "\n",
        "decoder_lstm1_e2d2 = tf.keras.layers.LSTM(16, return_sequences=True)\n",
        "decoder_outputs1_e2d2 = decoder_lstm1_e2d2(decoder_inputs_e2d2, initial_state=encoder_states1_e2d2)\n",
        "\n",
        "decoder_lstm2_e2d2 = tf.keras.layers.LSTM(16, return_sequences=True)\n",
        "decoder_outputs2_e2d2 = decoder_lstm2_e2d2(decoder_outputs1_e2d2, initial_state=encoder_states2_e2d2)\n",
        "\n",
        "decoder_dense_e2d2 = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(n_features))\n",
        "decoder_outputs_e2d2 = decoder_dense_e2d2(decoder_outputs2_e2d2)\n",
        "\n",
        "model_e2d2_lstm = tf.keras.models.Model(encoder_inputs_e2d2, decoder_outputs_e2d2)\n",
        "\n",
        "\n",
        "# 3. Compilation and Training (same as before)\n",
        "reduce_lr = tf.keras.callbacks.LearningRateScheduler(lambda x: 1e-3 * 0.90 ** x)\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "model_e1d1_lstm.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.Huber())\n",
        "history_e1d1_lstm = model_e1d1_lstm.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), batch_size=16, verbose=1, callbacks=[reduce_lr, early_stopping])\n",
        "\n",
        "model_e2d2_lstm.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.Huber())\n",
        "history_e2d2_lstm = model_e2d2_lstm.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), batch_size=16, verbose=1, callbacks=[reduce_lr, early_stopping])\n",
        "\n",
        "# 4. Prediction and Inverse Scaling (same as before, but use the correct model names)\n",
        "pred_e1d1_lstm = model_e1d1_lstm.predict(X_test)\n",
        "pred_e2d2_lstm = model_e2d2_lstm.predict(X_test)\n",
        "\n",
        "for index, i in enumerate(features):\n",
        "    scaler = scalers['scaler_' + i]\n",
        "    pred_e1d1_lstm[:, :, index] = scaler.inverse_transform(pred_e1d1_lstm[:, :, index])\n",
        "    pred_e2d2_lstm[:, :, index] = scaler.inverse_transform(pred_e2d2_lstm[:, :, index])\n",
        "    y_train[:, :, index] = scaler.inverse_transform(y_train[:, :, index])\n",
        "    y_test[:, :, index] = scaler.inverse_transform(y_test[:, :, index])\n",
        "\n",
        "# 5. Evaluation (same as before, but use the correct prediction variables)\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "y_true_all = y_test.flatten()\n",
        "\n",
        "y_pred_e1d1_all = pred_e1d1_lstm.flatten() # Use predictions from the LSTM models\n",
        "y_pred_e2d2_all = pred_e2d2_lstm.flatten()\n",
        "\n",
        "# ... (rest of the evaluation code remains the same, just use the _lstm variables)\n",
        "\n",
        "mae_e1d1 = mean_absolute_error(y_true_all, y_pred_e1d1_all)\n",
        "mse_e1d1 = mean_squared_error(y_true_all, y_pred_e1d1_all)\n",
        "rmse_e1d1 = np.sqrt(mse_e1d1)\n",
        "mape_e1d1 = mean_absolute_percentage_error(y_true_all, y_pred_e1d1_all)\n",
        "r2_e1d1 = r2_score(y_true_all, y_pred_e1d1_all)\n",
        "\n",
        "# Define/calculate mse_e2d2 here:\n",
        "mse_e2d2 = mean_squared_error(y_true_all, y_pred_e2d2_all)\n",
        "rmse_e2d2 = np.sqrt(mse_e2d2)\n",
        "mape_e2d2 = mean_absolute_percentage_error(y_true_all, y_pred_e2d2_all)\n",
        "r2_e2d2 = r2_score(y_true_all, y_pred_e2d2_all)\n",
        "\n",
        "# Display results in a structured table\n",
        "import pandas as pd\n",
        "results_df = pd.DataFrame({\n",
        "    \"Model\": [\"E1D1\", \"E2D2\"],\n",
        "    \"MAE\": [mae_e1d1, mae_e2d2],\n",
        "    \"MSE\": [mse_e1d1, mse_e2d2],\n",
        "    \"RMSE\": [rmse_e1d1, rmse_e2d2],\n",
        "    \"MAPE\": [mape_e1d1, mape_e2d2],\n",
        "    \"R2 Score\": [r2_e1d1, r2_e2d2]\n",
        "})\n",
        "\n",
        "print(results_df)\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Reshape y_test and pred_e1d1 to 2D\n",
        "y_test_2d = y_test.reshape(-1, y_test.shape[-1])  # Reshape to (samples * time steps, features)\n",
        "\n",
        "# Get prediction from the bilstm model\n",
        "# Instead of pred_e1d1, use pred_e1d1_lstm (output from your E1D1 LSTM model)\n",
        "pred_e1d1_lstm_2d = pred_e1d1_lstm.reshape(-1, pred_e1d1_lstm.shape[-1])  # Reshape using pred_e1d1_lstm\n",
        "\n",
        "# Normalize MSE using variance of actual data\n",
        "y_var = np.var(y_test_2d)  # Variance of true values\n",
        "# Use pred_e1d1_lstm_2d instead of pred_e1d1_2d\n",
        "normalized_mse_e1d1 = mean_squared_error(y_test_2d, pred_e1d1_lstm_2d) / y_var\n",
        "\n",
        "# ... (similarly, use pred_e2d2_lstm for E2D2 calculations) ...\n",
        "\n",
        "pred_e2d2_lstm_2d = pred_e2d2_lstm.reshape(-1, pred_e2d2_lstm.shape[-1])  # Reshape using pred_e2d2_lstm\n",
        "# Use pred_e2d2_lstm_2d instead of pred_e2d2_2d\n",
        "normalized_mse_e2d2 = mean_squared_error(y_test_2d, pred_e2d2_lstm_2d) / y_var\n",
        "\n",
        "\n",
        "print(\"Normalized MSE E1D1:\", normalized_mse_e1d1)\n",
        "print(\"Normalized MSE E2D2:\", normalized_mse_e2d2)"
      ],
      "metadata": {
        "id": "ggjSOhNwrdk_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}