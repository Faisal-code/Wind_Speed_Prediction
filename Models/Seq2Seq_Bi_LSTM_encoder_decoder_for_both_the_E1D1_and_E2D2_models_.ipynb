{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QRVNe8Sjq1mC"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# 1. Data Preparation\n",
    "df = pd.read_csv('dataset.csv')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.set_index('date', inplace=True)  # Set 'date' as index\n",
    "\n",
    "# Features to use\n",
    "features = ['temperature', 'humidity', 'wind_speed', 'power']\n",
    "df = df[features]\n",
    "\n",
    "# Scaling\n",
    "scalers = {}\n",
    "for column in df.columns:\n",
    "    scaler = MinMaxScaler()\n",
    "    df[column] = scaler.fit_transform(df[[column]])\n",
    "    scalers['scaler_' + column] = scaler\n",
    "\n",
    "# Define n_past, n_future, and n_features\n",
    "n_past = 10  # Number of past time steps to use\n",
    "n_future = 5  # Number of future time steps to predict\n",
    "n_features = len(features)\n",
    "\n",
    "# Create sequences (sliding window approach)\n",
    "X = []\n",
    "y = []\n",
    "for i in range(n_past, len(df) - n_future + 1):\n",
    "    X.append(df.iloc[i - n_past:i].values)\n",
    "    y.append(df.iloc[i:i + n_future].values)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Train/Test Split (Temporal split - 80:20)\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "\n",
    "# E1D1 Model with BiLSTM\n",
    "\n",
    "encoder_inputs = tf.keras.layers.Input(shape=(n_past, n_features))\n",
    "encoder_bilstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_state=True))(encoder_inputs)\n",
    "encoder_states1 = encoder_bilstm[1:]\n",
    "\n",
    "decoder_inputs = tf.keras.layers.RepeatVector(n_future)(encoder_bilstm[0])\n",
    "\n",
    "decoder_bilstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(decoder_inputs, initial_state=encoder_states1)\n",
    "decoder_outputs1 = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(n_features))(decoder_bilstm)\n",
    "\n",
    "model_e1d1_bilstm = tf.keras.models.Model(encoder_inputs, decoder_outputs1)\n",
    "\n",
    "# E2D2 Model with 2 layers of BiLSTM\n",
    "\n",
    "encoder_inputs = tf.keras.layers.Input(shape=(n_past, n_features))\n",
    "encoder_bilstm1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True, return_state=True))(encoder_inputs)\n",
    "encoder_states1 = encoder_bilstm1[1:]\n",
    "\n",
    "encoder_bilstm2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_state=True))(encoder_bilstm1[0])\n",
    "encoder_states2 = encoder_bilstm2[1:]\n",
    "\n",
    "decoder_inputs = tf.keras.layers.RepeatVector(n_future)(encoder_bilstm2[0])\n",
    "\n",
    "decoder_bilstm1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(decoder_inputs, initial_state=encoder_states1)\n",
    "decoder_bilstm2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(decoder_bilstm1, initial_state=encoder_states2)\n",
    "decoder_outputs2 = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(n_features))(decoder_bilstm2)\n",
    "\n",
    "model_e2d2_bilstm = tf.keras.models.Model(encoder_inputs, decoder_outputs2)\n",
    "\n",
    "# 3. Compilation and Training\n",
    "reduce_lr = tf.keras.callbacks.LearningRateScheduler(lambda x: 1e-3 * 0.90 ** x)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "model_e1d1_bilstm.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.Huber())\n",
    "history_e1d1_bilstm = model_e1d1_bilstm.fit(X_train, y_train, epochs=25, validation_data=(X_test, y_test), batch_size=16, verbose=1, callbacks=[reduce_lr, early_stopping])\n",
    "\n",
    "model_e2d2_bilstm.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.Huber())\n",
    "history_e2d2_bilstm = model_e2d2_bilstm.fit(X_train, y_train, epochs=25, validation_data=(X_test, y_test), batch_size=16, verbose=1, callbacks=[reduce_lr, early_stopping])\n",
    "\n",
    "\n",
    "# 4. Prediction and Inverse Scaling\n",
    "pred_e1d1_bilstm = model_e1d1_bilstm.predict(X_test)\n",
    "pred_e2d2_bilstm = model_e2d2_bilstm.predict(X_test)\n",
    "\n",
    "for index, i in enumerate(features):  # Iterate through the feature names\n",
    "    scaler = scalers['scaler_' + i]\n",
    "    pred_e1d1_bilstm[:, :, index] = scaler.inverse_transform(pred_e1d1_bilstm[:, :, index])\n",
    "    pred_e2d2_bilstm[:, :, index] = scaler.inverse_transform(pred_e2d2_bilstm[:, :, index])\n",
    "    y_train[:, :, index] = scaler.inverse_transform(y_train[:, :, index])\n",
    "    y_test[:, :, index] = scaler.inverse_transform(y_test[:, :, index])\n",
    "\n",
    "\n",
    "# 5. Evaluation (for the whole model across all 5 predicted days)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Define function for Mean Absolute Percentage Error (MAPE)\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# Flatten the true and predicted values for overall comparison\n",
    "y_true_all = y_test.flatten()\n",
    "# Use predictions from the BiLSTM models:\n",
    "y_pred_e1d1_all = pred_e1d1_bilstm.flatten()  # Changed from pred_e1d1 to pred_e1d1_bilstm\n",
    "y_pred_e2d2_all = pred_e2d2_bilstm.flatten()  # Changed from pred_e2d2 to pred_e2d2_bilstm\n",
    "\n",
    "# Compute metrics for E1D1 model\n",
    "mae_e1d1 = mean_absolute_error(y_true_all, y_pred_e1d1_all)\n",
    "mse_e1d1 = mean_squared_error(y_true_all, y_pred_e1d1_all)\n",
    "rmse_e1d1 = np.sqrt(mse_e1d1)\n",
    "mape_e1d1 = mean_absolute_percentage_error(y_true_all, y_pred_e1d1_all)\n",
    "r2_e1d1 = r2_score(y_true_all, y_pred_e1d1_all)\n",
    "\n",
    "# Compute metrics for E2D2 model\n",
    "mae_e2d2 = mean_absolute_error(y_true_all, y_pred_e2d2_all)\n",
    "mse_e2d2 = mean_squared_error(y_true_all, y_pred_e2d2_all)\n",
    "rmse_e2d2 = np.sqrt(mse_e2d2)\n",
    "r2_e2d2 = r2_score(y_true_all, y_pred_e2d2_all)\n",
    "\n",
    "# Display results in a structured table\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame({\n",
    "    \"Model\": [\"E1D1\", \"E2D2\"],\n",
    "    \"MAE\": [mae_e1d1, mae_e2d2],\n",
    "    \"MSE\": [mse_e1d1, mse_e2d2],\n",
    "    \"RMSE\": [rmse_e1d1, rmse_e2d2],\n",
    "    \"R2 Score\": [r2_e1d1, r2_e2d2]\n",
    "})\n",
    "\n",
    "print(results_df)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Reshape y_test and pred_e1d1 to 2D\n",
    "y_test_2d = y_test.reshape(-1, y_test.shape[-1])  # Reshape to (samples * time steps, features)\n",
    "#Get prediction from the bilstm model\n",
    "pred_e1d1_bilstm_2d = pred_e1d1_bilstm.reshape(-1, pred_e1d1_bilstm.shape[-1])  # Reshape to (samples * time steps, features)\n",
    "\n",
    "# Normalize MSE using variance of actual data\n",
    "y_var = np.var(y_test_2d)  # Variance of true values\n",
    "normalized_mse_e1d1 = mean_squared_error(y_test_2d, pred_e1d1_bilstm_2d) / y_var #Use the prediction from bilstm model\n",
    "\n",
    "# Reshape y_test and pred_e2d2 to 2D for E2D2 model\n",
    "pred_e2d2_bilstm_2d = pred_e2d2_bilstm.reshape(-1, pred_e2d2_bilstm.shape[-1])  # Reshape to (samples * time steps, features)\n",
    "normalized_mse_e2d2 = mean_squared_error(y_test_2d, pred_e2d2_bilstm_2d) / y_var #Use the prediction from bilstm model\n",
    "\n",
    "\n",
    "print(\"Normalized MSE E1D1:\", normalized_mse_e1d1)\n",
    "print(\"Normalized MSE E2D2:\", normalized_mse_e2d2)\n",
    "\n",
    "# Define the split_series function (re-added)\n",
    "def split_series(series, n_past, n_future):\n",
    "    X, y = list(), list()\n",
    "    for window_start in range(len(series)):\n",
    "        past_end = window_start + n_past\n",
    "        future_end = past_end + n_future\n",
    "        if future_end > len(series):\n",
    "            break\n",
    "        past, future = series[window_start:past_end, :], series[past_end:future_end, :]\n",
    "        X.append(past)\n",
    "        y.append(future)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "n_future = 10  # Predict 10 days ahead (already set in data prep, but good to reiterate)\n",
    "\n",
    "# Reshape X_test for single prediction\n",
    "X_test_last = X_test[-1:].reshape(1, n_past, n_features) # Reshape to (1, n_past, n_features)\n",
    "\n",
    "pred_next_10_days_e1d1 = model_e1d1_bilstm.predict(X_test_last)\n",
    "pred_next_10_days_e2d2 = model_e2d2_bilstm.predict(X_test_last)\n",
    "\n",
    "# Inverse transform the predicted values\n",
    "for index, i in enumerate(features):  # Use 'features' list\n",
    "    scaler = scalers['scaler_' + i]\n",
    "    pred_next_10_days_e1d1[:, :, index] = scaler.inverse_transform(pred_next_10_days_e1d1[:, :, index])\n",
    "    pred_next_10_days_e2d2[:, :, index] = scaler.inverse_transform(pred_next_10_days_e2d2[:, :, index])\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
